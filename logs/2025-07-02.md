# 📘 Log — *Why Machines Learn* by Anil Ananthaswamy  
**Date:** 2025-07-03  
**Chapter:** 1 — *The Perceptron and the Pattern of Learning*

---

## ✅ What I Did
- Finished reading **Chapter 1** of *Why Machines Learn*
- Took notes on perceptrons and their historical background

---

## 💡 What I Learned
- The **perceptron** is one of the earliest models in machine learning
- It finds **correlations** between inputs and outputs without understanding meaning
- Early systems like the **Mark I perceptron** could distinguish patterns (like "B" vs "G") based on shape alone
- A **single-layer perceptron** can only separate data if it's **linearly separable**
- The **weight vector `w`** defines a **hyperplane** that splits input space into two regions
  - In 2D → line  
  - In 3D → plane  
  - Higher dimensions → hyperplane

---

## 🧠 Concepts To Revisit
- Meaning and examples of **linearly separable** datasets
- **Vectors** and their geometric roles in machine learning
- Difference between **correlation-based learning** vs **reasoning**

---

## ⏭️ What's Next
- Start **Chapter 2** on multilayer networks and their evolution
- Explore how **backpropagation** solves the limitations of simple perceptrons
