# ğŸ“˜ Log â€” *Why Machines Learn* by Anil Ananthaswamy  
**Date:** 2025-07-03  
**Chapter:** 1 â€” *The Perceptron and the Pattern of Learning*

---

## âœ… What I Did
- Finished reading **Chapter 1** of *Why Machines Learn*
- Took notes on perceptrons and their historical background

---

## ğŸ’¡ What I Learned
- The **perceptron** is one of the earliest models in machine learning
- It finds **correlations** between inputs and outputs without understanding meaning
- Early systems like the **Mark I perceptron** could distinguish patterns (like "B" vs "G") based on shape alone
- A **single-layer perceptron** can only separate data if it's **linearly separable**
- The **weight vector `w`** defines a **hyperplane** that splits input space into two regions
  - In 2D â†’ line  
  - In 3D â†’ plane  
  - Higher dimensions â†’ hyperplane

---

## ğŸ§  Concepts To Revisit
- Meaning and examples of **linearly separable** datasets
- **Vectors** and their geometric roles in machine learning
- Difference between **correlation-based learning** vs **reasoning**

---

## â­ï¸ What's Next
- Start **Chapter 2** on multilayer networks and their evolution
- Explore how **backpropagation** solves the limitations of simple perceptrons
