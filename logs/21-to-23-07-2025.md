# üìò Log ‚Äî *Why Machines Learn* by Anil Ananthaswamy  
**Date:** 2025-07-10  
**Chapter:** 9 ‚Äî *The Man Who Set Back Deep Learning*

---

## ‚úÖ What I Did
- Read **Chapter 9** of *Why Machines Learn*  
- Focused on the role of **activation functions** in neural networks  
- Learned about **sigmoid curves**, their mathematical properties, and how functions can be treated as **vectors**

---

## üí° What I Learned
- The chapter title refers to **Marvin Minsky**, whose critique of perceptrons unintentionally delayed progress in deep learning  
- The **sigmoid function** is central to neural networks, introducing **nonlinearity** and enabling models to learn complex patterns  
- Sigmoid outputs values between 0 and 1 and has a smooth, S-shaped curve  
- The **derivative** of the sigmoid is simple, which makes it useful for **backpropagation**  
- Functions can be interpreted as **vectors in high-dimensional space**, allowing techniques from linear algebra to be applied to learning  
- Understanding the **geometry of functions** helps grasp how neural networks learn by adjusting weights and biases in vector space  

---

## üß† Concepts To Revisit
- How **sigmoid activation** helps introduce nonlinearity  
- Limitations of sigmoid: **vanishing gradients** in deep networks  
- Intuition behind treating **functions as vectors**  
- How activation functions affect the **shape of decision boundaries**

---

## ‚è≠Ô∏è What's Next
- Continue to **Chapter 10** to dive deeper into **backpropagation** and how gradients are used to train multilayer networks  
- Understand the mechanics of **weight updates**, **loss functions**, and the optimization process
