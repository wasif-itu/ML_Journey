# ğŸ“˜ Log â€” *Why Machines Learn* by Anil Ananthaswamy  
**Date:** 2025-07-03  
**Chapter:** 2 â€” *The Trouble with Perceptrons*

---

## âœ… What I Did
- Read **Chapter 2** of *Why Machines Learn*
- Reflected on limitations of single-layer perceptrons
- Learned about the historical backlash against neural networks

---

## ğŸ’¡ What I Learned
- Perceptrons **cannot solve problems** that are **not linearly separable**, like the **XOR** problem
- This limitation led to skepticism and halted neural network research for years (AI Winter)
- **Marvin Minsky and Seymour Papertâ€™s book** (1969) highlighted these flaws and influenced funding decisions
- The solution to these issues involves using **multilayer neural networks**, which can model more complex functions
- The **idea of hidden layers** emerged as a way to overcome the limitations of single-layer models

---

## ğŸ§  Concepts To Revisit
- Why XOR is not linearly separable
- Historical impact of Minsky & Papertâ€™s critique
- Intuition behind how **hidden layers** add expressive power

---

## â­ï¸ What's Next
- Begin **Chapter 3** to explore how **backpropagation** made multilayer networks trainable
- Understand how gradients are used to update weights through multiple layers
