# 📘 Log — *Why Machines Learn* by Anil Ananthaswamy  
**Date:** 2025-07-03  
**Chapter:** 2 — *The Trouble with Perceptrons*

---

## ✅ What I Did
- Read **Chapter 2** of *Why Machines Learn*
- Reflected on limitations of single-layer perceptrons
- Learned about the historical backlash against neural networks

---

## 💡 What I Learned
- Perceptrons **cannot solve problems** that are **not linearly separable**, like the **XOR** problem
- This limitation led to skepticism and halted neural network research for years (AI Winter)
- **Marvin Minsky and Seymour Papert’s book** (1969) highlighted these flaws and influenced funding decisions
- The solution to these issues involves using **multilayer neural networks**, which can model more complex functions
- The **idea of hidden layers** emerged as a way to overcome the limitations of single-layer models

---

## 🧠 Concepts To Revisit
- Why XOR is not linearly separable
- Historical impact of Minsky & Papert’s critique
- Intuition behind how **hidden layers** add expressive power

---

## ⏭️ What's Next
- Begin **Chapter 3** to explore how **backpropagation** made multilayer networks trainable
- Understand how gradients are used to update weights through multiple layers
