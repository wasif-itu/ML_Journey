# ğŸ“˜ Log â€” *Why Machines Learn* by Anil Ananthaswamy  
**Date:** 2025-07-06  
**Chapter:** 4 â€” *In All Probability*

---

## âœ… What I Did
- Read **Chapter 4** of *Why Machines Learn*  
- Reflected on the role of **probability and statistics** in machine learning  
- Explored the concepts of **expectation**, **distributions**, and their application to learning models  

---

## ğŸ’¡ What I Learned
- Machine learning models **donâ€™t deal in certainty** â€” they operate under uncertainty, and thatâ€™s where **probability theory** becomes essential  
- **Expectation** (expected value) helps summarize outcomes by weighting them with their probabilities  
- Understanding **distributions** (like Gaussian, Bernoulli, and categorical) is fundamental to modeling and predicting real-world data  
- **Bayesâ€™ Theorem** is the backbone of many ML algorithms, allowing us to **update beliefs** based on new evidence  
- Probabilistic thinking enables models to **generalize** better and make **more reliable predictions** from limited or noisy data  

---

## ğŸ§  Concepts To Revisit
- Mathematical derivation of **expectation** and **variance**  
- Differences between **discrete** and **continuous** probability distributions  
- Applications of **Bayesian inference** in ML models  

---

## â­ï¸ What's Next
- Continue to **Chapter 5** to explore how **information theory** connects to learning and prediction  
- Understand how **entropy** and **uncertainty reduction** shape the behavior of learning algorithms
