# 📘 Log — *Why Machines Learn* by Anil Ananthaswamy  
**Date:** 2025-07-06  
**Chapter:** 4 — *In All Probability*

---

## ✅ What I Did
- Read **Chapter 4** of *Why Machines Learn*  
- Reflected on the role of **probability and statistics** in machine learning  
- Explored the concepts of **expectation**, **distributions**, and their application to learning models  

---

## 💡 What I Learned
- Machine learning models **don’t deal in certainty** — they operate under uncertainty, and that’s where **probability theory** becomes essential  
- **Expectation** (expected value) helps summarize outcomes by weighting them with their probabilities  
- Understanding **distributions** (like Gaussian, Bernoulli, and categorical) is fundamental to modeling and predicting real-world data  
- **Bayes’ Theorem** is the backbone of many ML algorithms, allowing us to **update beliefs** based on new evidence  
- Probabilistic thinking enables models to **generalize** better and make **more reliable predictions** from limited or noisy data  

---

## 🧠 Concepts To Revisit
- Mathematical derivation of **expectation** and **variance**  
- Differences between **discrete** and **continuous** probability distributions  
- Applications of **Bayesian inference** in ML models  

---

## ⏭️ What's Next
- Continue to **Chapter 5** to explore how **information theory** connects to learning and prediction  
- Understand how **entropy** and **uncertainty reduction** shape the behavior of learning algorithms
