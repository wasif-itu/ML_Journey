# üìò Log ‚Äî *Why Machines Learn* by Anil Ananthaswamy  
**Date:** 2025-07-07  
**Chapter:** 6 ‚Äî *There‚Äôs Magic in Them Matrices*

---

## ‚úÖ What I Did
- Read **Chapter 6** of *Why Machines Learn*  
- Studied the concept of **Principal Component Analysis (PCA)**  
- Explored the mathematical intuition behind dimensionality reduction  

---

## üí° What I Learned
- **PCA** is a technique used to reduce the **dimensionality** of datasets while retaining as much **variance** (information) as possible  
- The core idea is to **transform correlated features** into a new set of **uncorrelated components** called **principal components**  
- These components are sorted by how much variance they capture from the data  
- PCA helps in:
  - **Data compression**
  - **Noise reduction**
  - **Visualization of high-dimensional data** (e.g., 2D plots of 100D data)
- PCA is powered by **linear algebra** ‚Äî specifically **eigenvectors and eigenvalues** of the covariance matrix  
- The first principal component points in the direction of **maximum variance**, the second is orthogonal to it, and so on  

---

## üß† Concepts To Revisit
- Geometric intuition behind **projecting data onto new axes**  
- How **covariance matrix** captures relationships between variables  
- The role of **eigen decomposition** in extracting principal components  
- When to **normalize or standardize data** before applying PCA  

---

## ‚è≠Ô∏è What's Next
- Begin **Chapter 7** to dive into **clustering** and unsupervised learning  
- Understand how algorithms like **k-means** uncover patterns and groupings in data
