# 📘 Log — *Why Machines Learn* by Anil Ananthaswamy  
**Date:** 2025-07-07  
**Chapter:** 5 — *Birds of a Feather*

---

## ✅ What I Did
- Read **Chapter 5** of *Why Machines Learn*  
- Learned the origins and inner workings of the **k-Nearest Neighbors (k-NN)** algorithm  
- Reflected on how **similarity** drives decision-making in early ML models  

---

## 💡 What I Learned
- The **k-NN algorithm** is based on a simple idea: similar data points tend to belong to the same class — hence the title *Birds of a Feather*  
- k-NN is a **lazy learner** — it doesn’t build a model ahead of time, instead it stores the entire dataset and compares new inputs at prediction time  
- It calculates **distances** (usually Euclidean) between the new data point and all training points to find the closest "k" neighbors  
- The majority class among these neighbors decides the classification  
- k-NN is powerful for its simplicity and interpretability, especially in low-dimensional, well-separated data  
- However, its performance can degrade in **high-dimensional spaces** or with **noisy data**  

---

## 🧠 Concepts To Revisit
- Effects of different **distance metrics** (e.g., Manhattan, cosine) on classification results  
- How to choose the best value of **k** using cross-validation  
- Role of **normalization** or **feature scaling** in improving k-NN accuracy  

---

## ⏭️ What's Next
- Move on to **Chapter 6** to understand **decision trees** and how they split data based on feature thresholds  
- Explore how decision boundaries can be shaped using hierarchical questions
