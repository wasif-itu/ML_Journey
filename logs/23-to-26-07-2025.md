# üìò Log ‚Äî *Why Machines Learn* by Anil Ananthaswamy  
**Date:** 2025-07-11  
**Chapter:** 10 ‚Äî *The Algorithm That Put Paid to a Persistent Myth*

---

## ‚úÖ What I Did
- Read **Chapter 10** of *Why Machines Learn*  
- Focused on the concept and workings of **backpropagation** in neural networks  
- Explored how this algorithm revived deep learning by making it possible to train multilayer models effectively  

---

## üí° What I Learned
- **Backpropagation** is the core algorithm used to train neural networks by efficiently computing **gradients**  
- The chapter explains how it **disproved the myth** that multilayer neural networks were impractical to train  
- Backprop works by applying the **chain rule** of calculus to propagate the error **backward** through the network, from output to input  
- This allows the network to update each weight in proportion to its contribution to the total error  
- It introduced a structured way to compute **partial derivatives** layer by layer, making gradient descent feasible for deep architectures  
- The success of backpropagation marked a turning point for deep learning ‚Äî enabling more complex and accurate models  

---

## üß† Concepts To Revisit
- Mathematical steps of **forward pass**, **loss calculation**, and **backward pass**  
- How the **chain rule** links outputs to earlier layers  
- Role of **learning rate** in updating weights  
- Challenges like **vanishing gradients** and how they affect deep networks  

---

## ‚è≠Ô∏è What's Next
- Move on to **Chapter 11** to explore how **neural networks learn internal representations**  
- Understand how backpropagation enables deeper layers to extract increasingly abstract features from data
